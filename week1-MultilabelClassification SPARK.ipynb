{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict tags on StackOverflow with linear models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment you will learn how to predict tags for posts from [StackOverflow](https://stackoverflow.com). To solve this task you will use multilabel classification approach.\n",
    "\n",
    "### Libraries\n",
    "\n",
    "In this task you will need the following libraries:\n",
    "- [Numpy](http://www.numpy.org) — a package for scientific computing.\n",
    "- [Pandas](https://pandas.pydata.org) — a library providing high-performance, easy-to-use data structures and data analysis tools for the Python\n",
    "- [scikit-learn](http://scikit-learn.org/stable/index.html) — a tool for data mining and data analysis.\n",
    "- [NLTK](http://www.nltk.org) — a platform to work with natural language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "The following cell will download all data required for this assignment into the folder `week1/data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from common.download_utils import download_week1_resources\n",
    "\n",
    "download_week1_resources()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading\n",
    "We will create a grader instace below and use it to collect your answers. Note that these outputs will be stored locally inside grader and will be uploaded to platform only after running submiting function in the last part of this assignment. If you want to make partial submission, you can run that cell any time you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, RegexTokenizer, StopWordsRemover\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression, OneVsRest\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('NLP2').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this and most of the following assignments you will need to use a list of stop words. It can be downloaded from *nltk*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task you will deal with a dataset of post titles from StackOverflow. You are provided a split to 3 sets: *train*, *validation* and *test*. All corpora (except for *test*) contain titles of the posts and corresponding tags (100 tags are available). The *test* set is provided for Coursera's grading and doesn't contain answers. Upload the corpora using *pandas* and look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = spark.read.csv('data/train.tsv',inferSchema=True,header=True,sep='\\t')\n",
    "validation=spark.read.csv('data/validation.tsv',inferSchema=True,header=True,sep='\\t')\n",
    "test = spark.read.csv('data/test.tsv',inferSchema=True,header=True,sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "from string import ascii_letters, digits\n",
    "\n",
    "from pyspark.sql.functions import regexp_replace, trim, col, lower\n",
    "def removePunctuation(column):\n",
    "    \"\"\"Removes punctuation, changes to lower case, and strips leading and trailing spaces.\n",
    "\n",
    "    Note:\n",
    "        Only spaces, letters, and numbers should be retained.  Other characters should should be\n",
    "        eliminated (e.g. it's becomes its).  Leading and trailing spaces should be removed after\n",
    "        punctuation is removed.\n",
    "\n",
    "    Args:\n",
    "        column (Column): A Column containing a sentence.\n",
    "\n",
    "    Returns:\n",
    "        Column: A Column named 'sentence' with clean-up operations applied.\n",
    "    \"\"\"\n",
    "    #df = sqlContext.sql(\"SELECT * FROM table\")\n",
    "    return lower(trim(regexp_replace(column,'[^A-Za-z0-9 ]+', ''))).alias('sentence')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Column title remove punctuation and to lower. respect # and + because of C# and c++\n",
    "train = train.withColumn('title', lower(regexp_replace(train.title, '[^A-Za-z0-9#+ ]+', ' ')))\n",
    "validation = validation.withColumn('title', lower(regexp_replace(validation.title, '[^A-Za-z0-9#+ ]+', ' ')))\n",
    "test = test.withColumn('title', lower(regexp_replace(test.title, '[^A-Za-z0-9#+ ]+', ' ')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.select('title','tags').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tags in null replace with NoTag \n",
    "train = train.withColumn('tags_1', when( train.tags.isNull(), \"['NoTag']\").otherwise(train.tags))\n",
    "validation = validation.withColumn('tags_1', when( validation.tags.isNull(), \"['NoTag']\").otherwise(validation.tags))\n",
    "#train=train.na.fill('NA')\n",
    "train.select('tags','tags_1').show(truncate=True, n=290)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.filter(\"tags is NULL\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation.select('tags','tags_1').show(truncate=True, n=290)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pipeline Tokenizers and remove stop words\n",
    "\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"title\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "regexTokenizer2 = RegexTokenizer(inputCol=\"tags_1\", outputCol=\"tags_2\", pattern=\"[^A-Za-z0-9#+]+\")\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "\n",
    "train_trans = Pipeline(stages=[regexTokenizer,regexTokenizer2,remover]).fit(train).transform(train)\n",
    "val_trans = Pipeline(stages=[regexTokenizer,regexTokenizer2, remover]).fit(validation).transform(validation)\n",
    "test_trans = Pipeline(stages=[regexTokenizer, remover]).fit(test).transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, *title* column contains titles of the posts and *tags* colum countains the tags. It could be noticed that a number of tags for a post is not fixed and could be as many as necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a more comfortable usage, initialize *X_train*, *X_val*, *X_test*, *y_train*, *y_val*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train=train_trans.select('filtered')\n",
    "X_val =val_trans.select('filtered')\n",
    "X_test=test_trans.select('filtered')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most known difficulties when working with natural data is that it's unstructured. For example, if you use it \"as is\" and extract tokens just by splitting the titles by whitespaces, you will see that there are many \"weird\" tokens like *3.5?*, *\"Flip*, etc. To prevent the problems, it's usually useful to prepare the data somehow. In this task you'll write a function, which will be also used in the other assignments. \n",
    "\n",
    "**Task 1 (TextPrepare).** Implement the function *text_prepare* following the instructions. After that, run the function *test_test_prepare* to test it on tiny cases and submit it to Coursera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train=train_trans.withColumn('tags_1',explode(split('tags',', ')))\n",
    "y_train=y_train.withColumn('tags_1',regexp_replace(y_train.tags_1, '[^A-Za-z0-9#+ ]+', ''))\n",
    "\n",
    "y_val=val_trans.withColumn('tags_1',explode(split('tags',', ')))\n",
    "y_val=y_val.withColumn('tags_1',regexp_replace(y_val.tags_1, '[^A-Za-z0-9#+ ]+', ''))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val.filter(\"tags is NULL\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val.select('filtered','tags','tags_1').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.groupBy(\"tags\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val.groupBy(\"tags\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a column label with thag categorized\n",
    "\"\"\"\n",
    "StringIndexer encodes a string column of labels to a column of label indices. The indices are in [0, numLabels), \n",
    "ordered by label frequencies, so the most frequent label gets index 0. The unseen labels will be put at index numLabels \n",
    "if user chooses to keep them. If the input column is numeric, we cast it to string and index the string values. \n",
    "When downstream pipeline components such as Estimator or Transformer make use of this string-indexed label, you must set \n",
    "the input column of the component to this string-indexed column name. In many cases, you can set the input column with \n",
    "setInputCol.\n",
    "\"\"\"\n",
    "label_stringIdx = StringIndexer(inputCol = \"tags\", outputCol = \"label\")\n",
    "\n",
    "dataset = label_stringIdx.fit(y_train).transform(y_train)\n",
    "dataset2 = label_stringIdx.fit(y_val).transform(y_val)\n",
    "\n",
    "dataset.show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2.show(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run your implementation for questions from file *text_prepare_tests.tsv* to earn the points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can preprocess the titles using function *text_prepare* and  making sure that the headers don't have bad symbols:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each tag and for each word calculate how many times they occur in the train corpus. \n",
    "\n",
    "**Task 2 (WordsTagsCount).** Find 3 most popular tags and 3 most popular words in the train data and submit the results to earn the points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dictionary of all tags from train corpus with their counts.\n",
    "tags_counts = {}\n",
    "# Dictionary of all words from train corpus with their counts.\n",
    "words_counts = {}\n",
    "\n",
    "df_dict = X_train.collect()\n",
    "for r in df_dict:\n",
    "    for word in r[0]:\n",
    "        try:\n",
    "            words_counts[word] += 1\n",
    "        except KeyError:\n",
    "            words_counts[word] = 1\n",
    "\n",
    "#words_counts=sorted(words_counts.items(), key = lambda item: item[1], reverse=True)\n",
    "\n",
    "df_tags=y_train.select('tags_1').collect()\n",
    "for r in df_tags:\n",
    "    for word in r:\n",
    "        try:\n",
    "            tags_counts[word] += 1\n",
    "        except KeyError:\n",
    "            tags_counts[word] = 1\n",
    "\n",
    "#tags_counts=sorted(tags_counts.items(), key = lambda item: item[1], reverse=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tags_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are assume that *tags_counts* and *words_counts* are dictionaries like `{'some_word_or_tag': frequency}`. After appllying the sorting procedure, results will be look like this: `[('most_popular_word_or_tag', frequency), ('less_popular_word_or_tag', frequency), ...]`. The grader gets the results in the following format (two comma-separated strings with line break):\n",
    "\n",
    "    tag1,tag2,tag3\n",
    "    word1,word2,word3\n",
    "\n",
    "Pay attention that in this assignment you should not submit frequencies or some additional information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_tags = sorted(tags_counts.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "most_common_words = sorted(words_counts.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "\n",
    "print('WordsTagsCount', '%s\\n%s' % (','.join(tag for tag, _ in most_common_tags), \n",
    "                                                ','.join(word for word, _ in most_common_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming text to a vector\n",
    "\n",
    "Machine Learning algorithms work with numeric data and we cannot use the provided text data \"as is\". There are many ways to transform text data to numeric vectors. In this task you will try to use two of them.\n",
    "\n",
    "#### Bag of words\n",
    "\n",
    "One of the well-known approaches is a *bag-of-words* representation. To create this transformation, follow the steps:\n",
    "1. Find *N* most popular words in train corpus and numerate them. Now we have a dictionary of the most popular words.\n",
    "2. For each title in the corpora create a zero vector with the dimension equals to *N*.\n",
    "3. For each text in the corpora iterate over words which are in the dictionary and increase by 1 the corresponding coordinate.\n",
    "\n",
    "Let's try to do it for a toy example. Imagine that we have *N* = 4 and the list of the most popular words is \n",
    "\n",
    "    ['hi', 'you', 'me', 'are']\n",
    "\n",
    "Then we need to numerate them, for example, like this: \n",
    "\n",
    "    {'hi': 0, 'you': 1, 'me': 2, 'are': 3}\n",
    "\n",
    "And we have the text, which we want to transform to the vector:\n",
    "\n",
    "    'hi how are you'\n",
    "\n",
    "For this text we create a corresponding zero vector \n",
    "\n",
    "    [0, 0, 0, 0]\n",
    "    \n",
    "And interate over all words, and if the word is in the dictionary, we increase the value of the corresponding position in the vector:\n",
    "\n",
    "    'hi':  [1, 0, 0, 0]\n",
    "    'how': [1, 0, 0, 0] # word 'how' is not in our dictionary\n",
    "    'are': [1, 0, 0, 1]\n",
    "    'you': [1, 1, 0, 1]\n",
    "\n",
    "The resulting vector will be \n",
    "\n",
    "    [1, 1, 0, 1]\n",
    "   \n",
    "Implement the described encoding in the function *my_bag_of_words* with the size of the dictionary equals to 5000. To find the most common words use train data. You can test your code using the function *test_my_bag_of_words*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bag to words \n",
    "DICT_SIZE = 50000\n",
    "# Input data: Each row is a bag of words with a ID.\n",
    "\n",
    "# fit a CountVectorizerModel from the corpus TRAIN DATASET\n",
    "cv = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", vocabSize=DICT_SIZE, minDF=1.0)\n",
    "model = cv.fit(dataset)\n",
    "result = model.transform(dataset)\n",
    "\n",
    "#VALIDATION DATASET\n",
    "cvv = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", vocabSize=DICT_SIZE, minDF=1.0)\n",
    "modelv = cvv.fit(dataset2)\n",
    "resultv = modelv.transform(dataset2)\n",
    "\n",
    "#result.select('filtered','features').show(truncate=False, n=2)\n",
    "result.select('features','filtered','label').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultv.select('features','filtered','label').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=DICT_SIZE)\n",
    "featurizedData = hashingTF.transform(dataset)#TRAIN\n",
    "featurizedDatav = hashingTF.transform(dataset2)#VALIDATION\n",
    "# alternatively, CountVectorizer can also be used to get term frequency vectors\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)#TRAIN\n",
    "idfModelv = idf.fit(featurizedDatav)#VALIDATION\n",
    "rescaledData = idfModel.transform(featurizedData)#TRAIN\n",
    "rescaledDatav = idfModelv.transform(featurizedDatav)#VALIDATION\n",
    "\n",
    "rescaledData.select(\"filtered\", \"features\",'label').show(truncate=False, n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rescaledDatav.select(\"filtered\", \"features\",'label').show(truncate=False, n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might notice, we transform the data to sparse representation, to store the useful information efficiently. There are many [types](https://docs.scipy.org/doc/scipy/reference/sparse.html) of such representations, however slkearn algorithms can work only with [csr](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html#scipy.sparse.csr_matrix) matrix, so we will use this one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3 (BagOfWords).** For the 11th row in *X_train_mybag* find how many non-zero elements it has. In this task the answer (variable *non_zero_elements_count*) should be a number, e.g. 20."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF\n",
    "\n",
    "The second approach extends the bag-of-words framework by taking into account total frequencies of words in the corpora. It helps to penalize too frequent words and provide better features space. \n",
    "\n",
    "Implement function *tfidf_features* using class [TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) from *scikit-learn*. Use *train* corpus to train a vectorizer. Don't forget to take a look into the arguments that you can pass to it. We suggest that you filter out too rare words (occur less than in 5 titles) and too frequent words (occur more than in 90% of the titles). Also, use bigrams along with unigrams in your vocabulary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the function *train_classifier* for training a classifier. In this task we suggest to use One-vs-Rest approach, which is implemented in [OneVsRestClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html) class. In this approach *k* classifiers (= number of tags) are trained. As a basic classifier, use [LogisticRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html). It is one of the simplest methods, but often it performs good enough in text classification tasks. It might take some time, because a number of classifiers to train is large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set=result.select('features','label')\n",
    "train_val=resultv.select('features','label')\n",
    "train,test=train_set.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val.filter(\"features is NULL or label is NULL\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.count())\n",
    "print(test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression, OneVsRest\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # instantiate the base classifier.\n",
    "    lr = LogisticRegression(maxIter=10, tol=1E-6, fitIntercept=True)\n",
    "\n",
    "    # instantiate the One Vs Rest Classifier.\n",
    "    ovr = OneVsRest(classifier=lr)\n",
    "\n",
    "    # train the multiclass model.\n",
    "    ovrModel = ovr.fit(train)\n",
    "\n",
    "    # score the model on test data.\n",
    "    predictions = ovrModel.transform(test)\n",
    "\n",
    "    # obtain evaluator.\n",
    "    evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the classification error on test data.\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))\n",
    "# $example off$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train a Random Forest model.\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=12,  maxDepth=5, maxMemoryInMB=1024)\n",
    "\n",
    "# Chain RF in a Pipeline\n",
    "pipeline = Pipeline(stages=[rf])\n",
    "\n",
    "# Train model.\n",
    "model = rf.fit(train)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_classifier(X_train, y_train):\n",
    "    \"\"\"\n",
    "      X_train, y_train — training data\n",
    "      \n",
    "      return: trained classifier\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create and fit LogisticRegression wraped into OneVsRestClassifier.\n",
    "    #classifier = OneVsRestClassifier(RidgeClassifier(normalize=True))\n",
    "    classifier=OneVsRestClassifier(LogisticRegression(penalty=\"l1\", tol=0.0001, C=10))\n",
    "    #classifier=OneVsRestClassifier(SVC(kernel=\"linear\"))\n",
    "    classifier.fit(X_train, y_train)\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the classifiers for different data transformations: *bag-of-words* and *tf-idf*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier_mybag = train_classifier(X_train_mybag, y_train)\n",
    "classifier_tfidf = train_classifier(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can create predictions for the data. You will need two types of predictions: labels and scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_val_predicted_labels_mybag = classifier_mybag.predict(X_val_mybag)\n",
    "y_val_predicted_scores_mybag = classifier_mybag.decision_function(X_val_mybag)\n",
    "\n",
    "y_val_predicted_labels_tfidf = classifier_tfidf.predict(X_val_tfidf)\n",
    "y_val_predicted_scores_tfidf = classifier_tfidf.decision_function(X_val_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now take a look at how classifier, which uses TF-IDF, works for a few examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_pred_inversed = mlb.inverse_transform(y_val_predicted_labels_tfidf)\n",
    "y_val_inversed = mlb.inverse_transform(y_val)\n",
    "for i in range(3):\n",
    "    print('Title:\\t{}\\nTrue labels:\\t{}\\nPredicted labels:\\t{}\\n\\n'.format(\n",
    "        X_val[i],\n",
    "        ','.join(y_val_inversed[i]),\n",
    "        ','.join(y_val_pred_inversed[i])\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we would need to compare the results of different predictions, e.g. to see whether TF-IDF transformation helps or to try different regularization techniques in logistic regression. For all these experiments, we need to setup evaluation procedure. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "To evaluate the results we will use several classification metrics:\n",
    " - [Accuracy](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)\n",
    " - [F1-score](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)\n",
    " - [Area under ROC-curve](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html)\n",
    " - [Area under precision-recall curve](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html#sklearn.metrics.average_precision_score) \n",
    " \n",
    "Make sure you are familiar with all of them. How would you expect the things work for the multi-label scenario? Read about micro/macro/weighted averaging following the sklearn links provided above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score \n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the function *print_evaluation_scores* which calculates and prints to stdout:\n",
    " - *accuracy*\n",
    " - *F1-score macro/micro/weighted*\n",
    " - *Precision macro/micro/weighted*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_evaluation_scores(y_val, predicted):\n",
    "    \n",
    "    print(accuracy_score(y_val, predicted))\n",
    "    print(f1_score(y_val, predicted, average='weighted'))\n",
    "    print(average_precision_score(y_val, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Bag-of-words')\n",
    "print_evaluation_scores(y_val, y_val_predicted_labels_mybag)\n",
    "print('Tfidf')\n",
    "print_evaluation_scores(y_val, y_val_predicted_labels_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might also want to plot some generalization of the [ROC curve](http://scikit-learn.org/stable/modules/model_evaluation.html#receiver-operating-characteristic-roc) for the case of multi-label classification. Provided function *roc_auc* can make it for you. The input parameters of this function are:\n",
    " - true labels\n",
    " - decision functions scores\n",
    " - number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from metrics import roc_auc\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = len(tags_counts)\n",
    "roc_auc(y_val, y_val_predicted_scores_mybag, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_classes = len(tags_counts)\n",
    "roc_auc(y_val, y_val_predicted_scores_tfidf, n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4 (MultilabelClassification).** Once we have the evaluation set up, we suggest that you experiment a bit with training your classifiers. We will use *F1-score weighted* as an evaluation metric. Our recommendation:\n",
    "- compare the quality of the bag-of-words and TF-IDF approaches and chose one of them.\n",
    "- for the chosen one, try *L1* and *L2*-regularization techniques in Logistic Regression with different coefficients (e.g. C equal to 0.1, 1, 10, 100).\n",
    "\n",
    "You also could try other improvements of the preprocessing / model, if you want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################################\n",
    "######### YOUR CODE HERE #############\n",
    "######################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are happy with the quality, create predictions for *test* set, which you will submit to Coursera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions =classifier_tfidf.predict(X_test_tfidf)\n",
    "test_pred_inversed = mlb.inverse_transform(test_predictions)\n",
    "\n",
    "test_predictions_for_submission = '\\n'.join('%i\\t%s' % (i, ','.join(row)) for i, row in enumerate(test_pred_inversed))\n",
    "grader.submit_tag('MultilabelClassification', test_predictions_for_submission)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of the most important features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, it is usually a good idea to look at the features (words or n-grams) that are used with the largest weigths in your logistic regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the function *print_words_for_tag* to find them. Get back to sklearn documentation on [OneVsRestClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html) and [LogisticRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_words_for_tag(classifier, tag, tags_classes, index_to_words, all_words):\n",
    "    \"\"\"\n",
    "        classifier: trained classifier\n",
    "        tag: particular tag\n",
    "        tags_classes: a list of classes names from MultiLabelBinarizer\n",
    "        index_to_words: index_to_words transformation\n",
    "        all_words: all words in the dictionary\n",
    "        \n",
    "        return nothing, just print top 5 positive and top 5 negative words for current tag\n",
    "    \"\"\"\n",
    "    print('Tag:\\t{}'.format(tag))\n",
    "    \n",
    "    # Extract an estimator from the classifier for the given tag.\n",
    "    # Extract feature coefficients from the estimator. \n",
    "    \n",
    "    estimator = classifier.estimators_[tags_classes.index(tag)]\n",
    "    top_positive_words = [index_to_words[index] for index in estimator.coef_.argsort().tolist()[0][-5:]]  # top-5 words sorted by the coefficiens.\n",
    "    top_negative_words = [index_to_words[index] for index in estimator.coef_.argsort().tolist()[0][:5]] # bottom-5 words  sorted by the coefficients.\n",
    "\n",
    "    print('Top positive words:\\t{}'.format(', '.join(top_positive_words)))\n",
    "    print('Top negative words:\\t{}\\n'.format(', '.join(top_negative_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_words_for_tag(classifier_tfidf, 'c', mlb.classes, tfidf_reversed_vocab, ALL_WORDS)\n",
    "print_words_for_tag(classifier_tfidf, 'c++', mlb.classes, tfidf_reversed_vocab, ALL_WORDS)\n",
    "print_words_for_tag(classifier_tfidf, 'linux', mlb.classes, tfidf_reversed_vocab, ALL_WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authorization & Submission\n",
    "To submit assignment parts to Cousera platform, please, enter your e-mail and token into variables below. You can generate token on this programming assignment page. <b>Note:</b> Token expires 30 minutes after generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader.status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STUDENT_EMAIL ='olonok@gmail.com' # EMAIL \n",
    "STUDENT_TOKEN = 'bRLk3LTM3vTrz7yI'\n",
    "grader.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to submit these answers, run cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader.submit(STUDENT_EMAIL, STUDENT_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
